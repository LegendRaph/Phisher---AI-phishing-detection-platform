{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f31c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99d5349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>url_length</th>\n",
       "      <th>number_of_dots_in_url</th>\n",
       "      <th>having_repeated_digits_in_url</th>\n",
       "      <th>number_of_digits_in_url</th>\n",
       "      <th>number_of_special_char_in_url</th>\n",
       "      <th>number_of_hyphens_in_url</th>\n",
       "      <th>number_of_underline_in_url</th>\n",
       "      <th>number_of_slash_in_url</th>\n",
       "      <th>number_of_questionmark_in_url</th>\n",
       "      <th>...</th>\n",
       "      <th>having_digits_in_subdomain</th>\n",
       "      <th>number_of_digits_in_subdomain</th>\n",
       "      <th>having_repeated_digits_in_subdomain</th>\n",
       "      <th>having_path</th>\n",
       "      <th>path_length</th>\n",
       "      <th>having_query</th>\n",
       "      <th>having_fragment</th>\n",
       "      <th>having_anchor</th>\n",
       "      <th>entropy_of_url</th>\n",
       "      <th>entropy_of_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.010412</td>\n",
       "      <td>2.751629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.089470</td>\n",
       "      <td>3.532573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.386016</td>\n",
       "      <td>3.344698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.221947</td>\n",
       "      <td>3.189898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.103538</td>\n",
       "      <td>2.952820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247945</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.758289</td>\n",
       "      <td>3.323231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247946</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.937093</td>\n",
       "      <td>3.026987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247947</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.813207</td>\n",
       "      <td>3.327820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247948</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.540173</td>\n",
       "      <td>3.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247949</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.035274</td>\n",
       "      <td>3.182006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247950 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type  url_length  number_of_dots_in_url  \\\n",
       "0          0          37                      2   \n",
       "1          1          70                      5   \n",
       "2          0          42                      2   \n",
       "3          0          46                      2   \n",
       "4          0          51                      3   \n",
       "...      ...         ...                    ...   \n",
       "247945     0          42                      1   \n",
       "247946     0          42                      2   \n",
       "247947     1          33                      2   \n",
       "247948     1          83                      1   \n",
       "247949     0          34                      3   \n",
       "\n",
       "        having_repeated_digits_in_url  number_of_digits_in_url  \\\n",
       "0                                   0                        0   \n",
       "1                                   0                        0   \n",
       "2                                   0                        6   \n",
       "3                                   0                        0   \n",
       "4                                   0                        0   \n",
       "...                               ...                      ...   \n",
       "247945                              0                        0   \n",
       "247946                              0                        0   \n",
       "247947                              0                        0   \n",
       "247948                              1                       19   \n",
       "247949                              0                        0   \n",
       "\n",
       "        number_of_special_char_in_url  number_of_hyphens_in_url  \\\n",
       "0                                   8                         0   \n",
       "1                                  12                         0   \n",
       "2                                   8                         0   \n",
       "3                                   7                         0   \n",
       "4                                   9                         0   \n",
       "...                               ...                       ...   \n",
       "247945                              6                         0   \n",
       "247946                              8                         0   \n",
       "247947                              8                         0   \n",
       "247948                              9                         0   \n",
       "247949                              7                         0   \n",
       "\n",
       "        number_of_underline_in_url  number_of_slash_in_url  \\\n",
       "0                                0                       5   \n",
       "1                                0                       6   \n",
       "2                                0                       3   \n",
       "3                                0                       4   \n",
       "4                                0                       5   \n",
       "...                            ...                     ...   \n",
       "247945                           0                       4   \n",
       "247946                           0                       5   \n",
       "247947                           0                       5   \n",
       "247948                           0                       7   \n",
       "247949                           0                       3   \n",
       "\n",
       "        number_of_questionmark_in_url  ...  having_digits_in_subdomain  \\\n",
       "0                                   0  ...                           0   \n",
       "1                                   0  ...                           0   \n",
       "2                                   1  ...                           0   \n",
       "3                                   0  ...                           0   \n",
       "4                                   0  ...                           0   \n",
       "...                               ...  ...                         ...   \n",
       "247945                              0  ...                           0   \n",
       "247946                              0  ...                           0   \n",
       "247947                              0  ...                           0   \n",
       "247948                              0  ...                           0   \n",
       "247949                              0  ...                           0   \n",
       "\n",
       "        number_of_digits_in_subdomain  having_repeated_digits_in_subdomain  \\\n",
       "0                                   0                                    1   \n",
       "1                                   0                                    1   \n",
       "2                                   0                                    1   \n",
       "3                                   0                                    1   \n",
       "4                                   0                                    1   \n",
       "...                               ...                                  ...   \n",
       "247945                              0                                    1   \n",
       "247946                              0                                    1   \n",
       "247947                              0                                    1   \n",
       "247948                              0                                    1   \n",
       "247949                              0                                    1   \n",
       "\n",
       "        having_path  path_length  having_query  having_fragment  \\\n",
       "0                 0            3             0                0   \n",
       "1                 0            4             0                0   \n",
       "2                 0            1             1                0   \n",
       "3                 0            2             0                0   \n",
       "4                 0            3             0                0   \n",
       "...             ...          ...           ...              ...   \n",
       "247945            0            2             0                0   \n",
       "247946            0            3             0                0   \n",
       "247947            0            3             0                0   \n",
       "247948            0            5             0                0   \n",
       "247949            0            1             0                0   \n",
       "\n",
       "        having_anchor  entropy_of_url  entropy_of_domain  \n",
       "0                   0        4.010412           2.751629  \n",
       "1                   0        4.089470           3.532573  \n",
       "2                   0        4.386016           3.344698  \n",
       "3                   0        4.221947           3.189898  \n",
       "4                   0        4.103538           2.952820  \n",
       "...               ...             ...                ...  \n",
       "247945              0        3.758289           3.323231  \n",
       "247946              0        3.937093           3.026987  \n",
       "247947              0        3.813207           3.327820  \n",
       "247948              0        4.540173           3.375000  \n",
       "247949              0        4.035274           3.182006  \n",
       "\n",
       "[247950 rows x 42 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset.zip\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bbf409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([247950, 41]), torch.Size([247950]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(\"Type\", axis=1)\n",
    "y = df[\"Type\"]\n",
    "\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d06a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([198360, 41])  |   X_test shape: torch.Size([49590, 41])\n",
      "y_train shape: torch.Size([198360])      |   y_test shape: torch.Size([49590])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}  |   X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}      |   y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32eff286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer):\n",
    "        super(PhishingClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_layer, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.relu(self.layer_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f86e7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhishingClassifier(\n",
       "  (layer_1): Linear(in_features=41, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (layer_2): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = PhishingClassifier(input_size=X.shape[1], hidden_layer=32)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c4dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy block\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7efa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3754],\n",
       "        [2.9292],\n",
       "        [4.7695],\n",
       "        ...,\n",
       "        [4.7660],\n",
       "        [6.5536],\n",
       "        [2.4911]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    initial_pred = model_0(X_test)\n",
    "initial_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2f322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loos function and Optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed883bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.0590240955352783\n",
      "Accuracy: 48.138233514821536\n",
      "Epoch: 10 | Loss: 0.6385346055030823\n",
      "Accuracy: 64.42226255293406\n",
      "Epoch: 20 | Loss: 0.5984965562820435\n",
      "Accuracy: 66.49879007864489\n",
      "Epoch: 30 | Loss: 0.5807524919509888\n",
      "Accuracy: 68.41802782819116\n",
      "Epoch: 40 | Loss: 0.569290041923523\n",
      "Accuracy: 70.32163742690058\n",
      "Epoch: 50 | Loss: 0.5614228844642639\n",
      "Accuracy: 71.56130268199233\n",
      "Epoch: 60 | Loss: 0.5552595853805542\n",
      "Accuracy: 72.08862673926195\n",
      "Epoch: 70 | Loss: 0.5524211525917053\n",
      "Accuracy: 72.12542851381328\n",
      "Epoch: 80 | Loss: 0.5709681510925293\n",
      "Accuracy: 68.82990522282718\n",
      "Epoch: 90 | Loss: 0.5768841505050659\n",
      "Accuracy: 68.15739060294415\n",
      "Epoch: 100 | Loss: 0.5521175265312195\n",
      "Accuracy: 71.25378100423472\n",
      "Epoch: 110 | Loss: 0.5427201986312866\n",
      "Accuracy: 72.35682597297843\n",
      "Epoch: 120 | Loss: 0.54136061668396\n",
      "Accuracy: 72.376487194999\n",
      "Epoch: 130 | Loss: 0.5439746379852295\n",
      "Accuracy: 71.79068360556565\n",
      "Epoch: 140 | Loss: 0.5415635108947754\n",
      "Accuracy: 72.04980842911878\n",
      "Epoch: 150 | Loss: 0.5354355573654175\n",
      "Accuracy: 72.82365396249244\n",
      "Epoch: 160 | Loss: 0.5312777161598206\n",
      "Accuracy: 73.23250655374068\n",
      "Epoch: 170 | Loss: 0.5293626189231873\n",
      "Accuracy: 73.36156483161928\n",
      "Epoch: 180 | Loss: 0.5275185704231262\n",
      "Accuracy: 73.54204476709015\n",
      "Epoch: 190 | Loss: 0.5252696871757507\n",
      "Accuracy: 73.76890502117362\n",
      "Epoch: 200 | Loss: 0.5230793952941895\n",
      "Accuracy: 73.94888082274652\n",
      "Epoch: 210 | Loss: 0.52008056640625\n",
      "Accuracy: 74.23926194797338\n",
      "Epoch: 220 | Loss: 0.5174306035041809\n",
      "Accuracy: 74.51199838677152\n",
      "Epoch: 230 | Loss: 0.5160197019577026\n",
      "Accuracy: 74.61030449687436\n",
      "Epoch: 240 | Loss: 0.5140528082847595\n",
      "Accuracy: 74.78624722726356\n",
      "Epoch: 250 | Loss: 0.5115600228309631\n",
      "Accuracy: 75.12653760838879\n",
      "Epoch: 260 | Loss: 0.5102090239524841\n",
      "Accuracy: 75.35138132688041\n",
      "Epoch: 270 | Loss: 0.5083709359169006\n",
      "Accuracy: 75.48800161322848\n",
      "Epoch: 280 | Loss: 0.5058624744415283\n",
      "Accuracy: 75.64277071990321\n",
      "Epoch: 290 | Loss: 0.5042604804039001\n",
      "Accuracy: 75.62966323855616\n",
      "Epoch: 300 | Loss: 0.5031142830848694\n",
      "Accuracy: 75.70729985884252\n",
      "Epoch: 310 | Loss: 0.5016452074050903\n",
      "Accuracy: 75.79804396047591\n",
      "Epoch: 320 | Loss: 0.5001702904701233\n",
      "Accuracy: 75.88072191974187\n",
      "Epoch: 330 | Loss: 0.4986281991004944\n",
      "Accuracy: 75.9765073603549\n",
      "Epoch: 340 | Loss: 0.49684348702430725\n",
      "Accuracy: 76.14791288566242\n",
      "Epoch: 350 | Loss: 0.4949829876422882\n",
      "Accuracy: 76.25680580762251\n",
      "Epoch: 360 | Loss: 0.4936767816543579\n",
      "Accuracy: 76.3213349465618\n",
      "Epoch: 370 | Loss: 0.49187415838241577\n",
      "Accuracy: 76.46602137527726\n",
      "Epoch: 380 | Loss: 0.4905964434146881\n",
      "Accuracy: 76.54567453115547\n",
      "Epoch: 390 | Loss: 0.4880174398422241\n",
      "Accuracy: 76.84462593264772\n",
      "Epoch: 400 | Loss: 0.48534873127937317\n",
      "Accuracy: 77.03115547489413\n",
      "Epoch: 410 | Loss: 0.48378369212150574\n",
      "Accuracy: 77.14408146803791\n",
      "Epoch: 420 | Loss: 0.48242345452308655\n",
      "Accuracy: 77.24238757814076\n",
      "Epoch: 430 | Loss: 0.4806605577468872\n",
      "Accuracy: 77.33313167977414\n",
      "Epoch: 440 | Loss: 0.4794321358203888\n",
      "Accuracy: 77.41379310344828\n",
      "Epoch: 450 | Loss: 0.4784189462661743\n",
      "Accuracy: 77.49647106271425\n",
      "Epoch: 460 | Loss: 0.4780559837818146\n",
      "Accuracy: 77.52974389997983\n",
      "Epoch: 470 | Loss: 0.4776914715766907\n",
      "Accuracy: 77.55696713046986\n",
      "Epoch: 480 | Loss: 0.47689127922058105\n",
      "Accuracy: 77.60838878806211\n",
      "Epoch: 490 | Loss: 0.47630879282951355\n",
      "Accuracy: 77.6401492236338\n",
      "Epoch: 500 | Loss: 0.4758230745792389\n",
      "Accuracy: 77.66586005242992\n",
      "Epoch: 510 | Loss: 0.4752781093120575\n",
      "Accuracy: 77.70114942528735\n",
      "Epoch: 520 | Loss: 0.4744186997413635\n",
      "Accuracy: 77.76114135914499\n",
      "Epoch: 530 | Loss: 0.47363847494125366\n",
      "Accuracy: 77.80096793708408\n",
      "Epoch: 540 | Loss: 0.47320568561553955\n",
      "Accuracy: 77.83625730994153\n",
      "Epoch: 550 | Loss: 0.47255241870880127\n",
      "Accuracy: 77.88011695906432\n",
      "Epoch: 560 | Loss: 0.4719465672969818\n",
      "Accuracy: 77.9355716878403\n",
      "Epoch: 570 | Loss: 0.4711691737174988\n",
      "Accuracy: 77.9890098810244\n",
      "Epoch: 580 | Loss: 0.47039663791656494\n",
      "Accuracy: 78.03387779794313\n",
      "Epoch: 590 | Loss: 0.46990957856178284\n",
      "Accuracy: 78.06513409961686\n",
      "Epoch: 600 | Loss: 0.46899548172950745\n",
      "Accuracy: 78.12714256906634\n",
      "Epoch: 610 | Loss: 0.46843278408050537\n",
      "Accuracy: 78.16344020971971\n",
      "Epoch: 620 | Loss: 0.4682609438896179\n",
      "Accuracy: 78.17049808429118\n",
      "Epoch: 630 | Loss: 0.46773016452789307\n",
      "Accuracy: 78.19973785037307\n",
      "Epoch: 640 | Loss: 0.4667190611362457\n",
      "Accuracy: 78.2486388384755\n",
      "Epoch: 650 | Loss: 0.4662567377090454\n",
      "Accuracy: 78.28191167574107\n",
      "Epoch: 660 | Loss: 0.46582114696502686\n",
      "Accuracy: 78.30157289776164\n",
      "Epoch: 670 | Loss: 0.4649161100387573\n",
      "Accuracy: 78.33887880621093\n",
      "Epoch: 680 | Loss: 0.46431800723075867\n",
      "Accuracy: 78.36610203670095\n",
      "Epoch: 690 | Loss: 0.46423864364624023\n",
      "Accuracy: 78.38072191974189\n",
      "Epoch: 700 | Loss: 0.4637967348098755\n",
      "Accuracy: 78.3872756604154\n",
      "Epoch: 710 | Loss: 0.4627563953399658\n",
      "Accuracy: 78.44474692478323\n",
      "Epoch: 720 | Loss: 0.46216800808906555\n",
      "Accuracy: 78.46793708408953\n",
      "Epoch: 730 | Loss: 0.4616038501262665\n",
      "Accuracy: 78.49818511796734\n",
      "Epoch: 740 | Loss: 0.4614480435848236\n",
      "Accuracy: 78.50877192982456\n",
      "Epoch: 750 | Loss: 0.46104922890663147\n",
      "Accuracy: 78.5344827586207\n",
      "Epoch: 760 | Loss: 0.460783451795578\n",
      "Accuracy: 78.55162331115145\n",
      "Epoch: 770 | Loss: 0.45987460017204285\n",
      "Accuracy: 78.5919540229885\n",
      "Epoch: 780 | Loss: 0.4592456519603729\n",
      "Accuracy: 78.62472272635613\n",
      "Epoch: 790 | Loss: 0.45933955907821655\n",
      "Accuracy: 78.61766485178462\n",
      "Epoch: 800 | Loss: 0.45900315046310425\n",
      "Accuracy: 78.64791288566244\n",
      "Epoch: 810 | Loss: 0.45810985565185547\n",
      "Accuracy: 78.68723532970357\n",
      "Epoch: 820 | Loss: 0.4577855169773102\n",
      "Accuracy: 78.69328493647913\n",
      "Epoch: 830 | Loss: 0.45723092555999756\n",
      "Accuracy: 78.71949989917321\n",
      "Epoch: 840 | Loss: 0.4569648206233978\n",
      "Accuracy: 78.74924379915306\n",
      "Epoch: 850 | Loss: 0.4564541280269623\n",
      "Accuracy: 78.77445049405122\n",
      "Epoch: 860 | Loss: 0.45593327283859253\n",
      "Accuracy: 78.79612825166365\n",
      "Epoch: 870 | Loss: 0.4556685984134674\n",
      "Accuracy: 78.8157894736842\n",
      "Epoch: 880 | Loss: 0.4550957679748535\n",
      "Accuracy: 78.83645896350072\n",
      "Epoch: 890 | Loss: 0.4545228183269501\n",
      "Accuracy: 78.85511191772535\n",
      "Epoch: 900 | Loss: 0.4540722966194153\n",
      "Accuracy: 78.88435168380722\n",
      "Epoch: 910 | Loss: 0.45357635617256165\n",
      "Accuracy: 78.91006251260335\n",
      "Epoch: 920 | Loss: 0.45301586389541626\n",
      "Accuracy: 78.93879814478726\n",
      "Epoch: 930 | Loss: 0.4526315927505493\n",
      "Accuracy: 78.96501310748135\n",
      "Epoch: 940 | Loss: 0.45250996947288513\n",
      "Accuracy: 78.97963299052229\n",
      "Epoch: 950 | Loss: 0.4515693485736847\n",
      "Accuracy: 79.0260133091349\n",
      "Epoch: 960 | Loss: 0.45106056332588196\n",
      "Accuracy: 79.04718693284937\n",
      "Epoch: 970 | Loss: 0.4509490728378296\n",
      "Accuracy: 79.05323653962493\n",
      "Epoch: 980 | Loss: 0.4505864977836609\n",
      "Accuracy: 79.06836055656382\n",
      "Epoch: 990 | Loss: 0.4497363269329071\n",
      "Accuracy: 79.10717886670699\n",
      "Epoch: 1000 | Loss: 0.449250727891922\n",
      "Accuracy: 79.11373260738051\n",
      "Epoch: 1010 | Loss: 0.4490930438041687\n",
      "Accuracy: 79.1056664650131\n",
      "Epoch: 1020 | Loss: 0.44869354367256165\n",
      "Accuracy: 79.11625327687034\n",
      "Epoch: 1030 | Loss: 0.4489900469779968\n",
      "Accuracy: 79.11171607178866\n",
      "Epoch: 1040 | Loss: 0.44873055815696716\n",
      "Accuracy: 79.12028634805405\n",
      "Epoch: 1050 | Loss: 0.4479481279850006\n",
      "Accuracy: 79.15003024803387\n",
      "Epoch: 1060 | Loss: 0.4474698305130005\n",
      "Accuracy: 79.16515426497278\n",
      "Epoch: 1070 | Loss: 0.44744762778282166\n",
      "Accuracy: 79.162129461585\n",
      "Epoch: 1080 | Loss: 0.4466809034347534\n",
      "Accuracy: 79.19540229885057\n",
      "Epoch: 1090 | Loss: 0.4466310143470764\n",
      "Accuracy: 79.1868320225852\n",
      "Epoch: 1100 | Loss: 0.44647228717803955\n",
      "Accuracy: 79.18985682597298\n",
      "Epoch: 1110 | Loss: 0.44545236229896545\n",
      "Accuracy: 79.23573301068764\n",
      "Epoch: 1120 | Loss: 0.4454768896102905\n",
      "Accuracy: 79.24228675136116\n",
      "Epoch: 1130 | Loss: 0.44576191902160645\n",
      "Accuracy: 79.22565033272836\n",
      "Epoch: 1140 | Loss: 0.44513267278671265\n",
      "Accuracy: 79.26749344625932\n",
      "Epoch: 1150 | Loss: 0.4445674419403076\n",
      "Accuracy: 79.29471667674935\n",
      "Epoch: 1160 | Loss: 0.44451263546943665\n",
      "Accuracy: 79.28866706997378\n",
      "Epoch: 1170 | Loss: 0.4441298246383667\n",
      "Accuracy: 79.31236136317807\n",
      "Epoch: 1180 | Loss: 0.4435155391693115\n",
      "Accuracy: 79.3501714055253\n",
      "Epoch: 1190 | Loss: 0.4435131847858429\n",
      "Accuracy: 79.340592861464\n",
      "Epoch: 1200 | Loss: 0.4435059428215027\n",
      "Accuracy: 79.34311353095383\n",
      "Epoch: 1210 | Loss: 0.4430515468120575\n",
      "Accuracy: 79.36983262754588\n",
      "Epoch: 1220 | Loss: 0.44266247749328613\n",
      "Accuracy: 79.39352692075015\n",
      "Epoch: 1230 | Loss: 0.44242334365844727\n",
      "Accuracy: 79.40008066142367\n",
      "Epoch: 1240 | Loss: 0.44221892952919006\n",
      "Accuracy: 79.39907239362775\n",
      "Epoch: 1250 | Loss: 0.44202911853790283\n",
      "Accuracy: 79.40814680379108\n",
      "Epoch: 1260 | Loss: 0.44149330258369446\n",
      "Accuracy: 79.4338576325872\n",
      "Epoch: 1270 | Loss: 0.44118183851242065\n",
      "Accuracy: 79.44746924783223\n",
      "Epoch: 1280 | Loss: 0.4408584237098694\n",
      "Accuracy: 79.46259326477112\n",
      "Epoch: 1290 | Loss: 0.440552294254303\n",
      "Accuracy: 79.4656180681589\n",
      "Epoch: 1300 | Loss: 0.4403734505176544\n",
      "Accuracy: 79.4671304698528\n",
      "Epoch: 1310 | Loss: 0.4400131404399872\n",
      "Accuracy: 79.48175035289373\n",
      "Epoch: 1320 | Loss: 0.4397149682044983\n",
      "Accuracy: 79.49233716475096\n",
      "Epoch: 1330 | Loss: 0.43940165638923645\n",
      "Accuracy: 79.51250252066949\n",
      "Epoch: 1340 | Loss: 0.4393044114112854\n",
      "Accuracy: 79.52208106473078\n",
      "Epoch: 1350 | Loss: 0.4388066232204437\n",
      "Accuracy: 79.54829602742488\n",
      "Epoch: 1360 | Loss: 0.4384649395942688\n",
      "Accuracy: 79.57299858842508\n",
      "Epoch: 1370 | Loss: 0.43830692768096924\n",
      "Accuracy: 79.5755192579149\n",
      "Epoch: 1380 | Loss: 0.4380737245082855\n",
      "Accuracy: 79.58408953418028\n",
      "Epoch: 1390 | Loss: 0.4379211962223053\n",
      "Accuracy: 79.59114740875177\n",
      "Epoch: 1400 | Loss: 0.437630295753479\n",
      "Accuracy: 79.60223835450697\n",
      "Epoch: 1410 | Loss: 0.43765518069267273\n",
      "Accuracy: 79.61080863077234\n",
      "Epoch: 1420 | Loss: 0.43720147013664246\n",
      "Accuracy: 79.62946158499697\n",
      "Epoch: 1430 | Loss: 0.43692606687545776\n",
      "Accuracy: 79.64055253075216\n",
      "Epoch: 1440 | Loss: 0.4367886185646057\n",
      "Accuracy: 79.64761040532366\n",
      "Epoch: 1450 | Loss: 0.4364480972290039\n",
      "Accuracy: 79.65164347650736\n",
      "Epoch: 1460 | Loss: 0.4362379014492035\n",
      "Accuracy: 79.6607178866707\n",
      "Epoch: 1470 | Loss: 0.4360277056694031\n",
      "Accuracy: 79.6758419036096\n",
      "Epoch: 1480 | Loss: 0.43567654490470886\n",
      "Accuracy: 79.6773543053035\n",
      "Epoch: 1490 | Loss: 0.4353208541870117\n",
      "Accuracy: 79.69197418834443\n",
      "Epoch: 1500 | Loss: 0.4352462589740753\n",
      "Accuracy: 79.7005444646098\n",
      "Epoch: 1510 | Loss: 0.43483880162239075\n",
      "Accuracy: 79.70911474087518\n",
      "Epoch: 1520 | Loss: 0.43465203046798706\n",
      "Accuracy: 79.72222222222223\n",
      "Epoch: 1530 | Loss: 0.43468353152275085\n",
      "Accuracy: 79.71415607985482\n",
      "Epoch: 1540 | Loss: 0.4342971742153168\n",
      "Accuracy: 79.72474289171204\n",
      "Epoch: 1550 | Loss: 0.4340645968914032\n",
      "Accuracy: 79.74490824763058\n",
      "Epoch: 1560 | Loss: 0.4338267147541046\n",
      "Accuracy: 79.7554950594878\n",
      "Epoch: 1570 | Loss: 0.4336611330509186\n",
      "Accuracy: 79.76709013914095\n",
      "Epoch: 1580 | Loss: 0.4332334101200104\n",
      "Accuracy: 79.78120588828392\n",
      "Epoch: 1590 | Loss: 0.4328881502151489\n",
      "Accuracy: 79.81044565436581\n",
      "Epoch: 1600 | Loss: 0.43263572454452515\n",
      "Accuracy: 79.81044565436581\n",
      "Epoch: 1610 | Loss: 0.4325656592845917\n",
      "Accuracy: 79.82052833232507\n",
      "Epoch: 1620 | Loss: 0.4320412874221802\n",
      "Accuracy: 79.84674329501917\n",
      "Epoch: 1630 | Loss: 0.4317851662635803\n",
      "Accuracy: 79.8543053034886\n",
      "Epoch: 1640 | Loss: 0.4316229820251465\n",
      "Accuracy: 79.85884250857028\n",
      "Epoch: 1650 | Loss: 0.4312419593334198\n",
      "Accuracy: 79.8653962492438\n",
      "Epoch: 1660 | Loss: 0.4310244917869568\n",
      "Accuracy: 79.87194998991733\n",
      "Epoch: 1670 | Loss: 0.4308183491230011\n",
      "Accuracy: 79.8749747933051\n",
      "Epoch: 1680 | Loss: 0.4305266737937927\n",
      "Accuracy: 79.88656987295826\n",
      "Epoch: 1690 | Loss: 0.4298977851867676\n",
      "Accuracy: 79.9107683000605\n",
      "Epoch: 1700 | Loss: 0.4298747777938843\n",
      "Accuracy: 79.91026416616253\n",
      "Epoch: 1710 | Loss: 0.42980021238327026\n",
      "Accuracy: 79.91228070175438\n",
      "Epoch: 1720 | Loss: 0.42953333258628845\n",
      "Accuracy: 79.91480137124421\n",
      "Epoch: 1730 | Loss: 0.429269939661026\n",
      "Accuracy: 79.92690058479532\n",
      "Epoch: 1740 | Loss: 0.4290192723274231\n",
      "Accuracy: 79.9379915305505\n",
      "Epoch: 1750 | Loss: 0.4287627637386322\n",
      "Accuracy: 79.95614035087719\n",
      "Epoch: 1760 | Loss: 0.4285804033279419\n",
      "Accuracy: 79.95765275257108\n",
      "Epoch: 1770 | Loss: 0.4284506142139435\n",
      "Accuracy: 79.95563621697923\n",
      "Epoch: 1780 | Loss: 0.42821940779685974\n",
      "Accuracy: 79.9657188949385\n",
      "Epoch: 1790 | Loss: 0.4280882179737091\n",
      "Accuracy: 79.9697519661222\n",
      "Epoch: 1800 | Loss: 0.42791759967803955\n",
      "Accuracy: 79.97731397459165\n",
      "Epoch: 1810 | Loss: 0.4279072880744934\n",
      "Accuracy: 79.98386771526518\n",
      "Epoch: 1820 | Loss: 0.4276127219200134\n",
      "Accuracy: 79.9793305101835\n",
      "Epoch: 1830 | Loss: 0.427542120218277\n",
      "Accuracy: 79.98235531357129\n",
      "Epoch: 1840 | Loss: 0.4273926615715027\n",
      "Accuracy: 79.97630570679573\n",
      "Epoch: 1850 | Loss: 0.4271942377090454\n",
      "Accuracy: 79.98235531357129\n",
      "Epoch: 1860 | Loss: 0.426946759223938\n",
      "Accuracy: 79.9904214559387\n",
      "Epoch: 1870 | Loss: 0.426734060049057\n",
      "Accuracy: 79.99294212542851\n",
      "Epoch: 1880 | Loss: 0.42657333612442017\n",
      "Accuracy: 79.98840492034685\n",
      "Epoch: 1890 | Loss: 0.4263235628604889\n",
      "Accuracy: 79.99697519661223\n",
      "Epoch: 1900 | Loss: 0.42617976665496826\n",
      "Accuracy: 80.00100826779592\n",
      "Epoch: 1910 | Loss: 0.42598384618759155\n",
      "Accuracy: 80.01159507965315\n",
      "Epoch: 1920 | Loss: 0.42577919363975525\n",
      "Accuracy: 80.01058681185724\n",
      "Epoch: 1930 | Loss: 0.4254729449748993\n",
      "Accuracy: 80.02117362371446\n",
      "Epoch: 1940 | Loss: 0.42521193623542786\n",
      "Accuracy: 80.03781004234725\n",
      "Epoch: 1950 | Loss: 0.42496243119239807\n",
      "Accuracy: 80.0398265779391\n",
      "Epoch: 1960 | Loss: 0.42474600672721863\n",
      "Accuracy: 80.03680177455132\n",
      "Epoch: 1970 | Loss: 0.42470583319664\n",
      "Accuracy: 80.02873563218391\n",
      "Epoch: 1980 | Loss: 0.424441397190094\n",
      "Accuracy: 80.03226456946965\n",
      "Epoch: 1990 | Loss: 0.4241434335708618\n",
      "Accuracy: 80.041338979633\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "torch.manual_seed(42)\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    # Forward Pass\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "    # Loss\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    # Training Accuracy\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Back Propagation\n",
    "    loss.backward()\n",
    "    # Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss}\")\n",
    "        print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b0c7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([0., 1., 0.,  ..., 0., 0., 0.])\n",
      "Accuracy: 79.81044565436581\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    new_pred = torch.round(torch.sigmoid(model_0(X_test).squeeze()))\n",
    "    ac = accuracy_fn(y_true=y_test, y_pred=new_pred)\n",
    "\n",
    "print(f\"Prediction: {new_pred}\\nAccuracy: {ac}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a381074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), f='Phisher Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d2598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    counts = Counter(text)\n",
    "    probs = [c / len(text) for c in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_ordered(url: str):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path\n",
    "    query = parsed.query\n",
    "    fragment = parsed.fragment\n",
    "\n",
    "    # Special characters\n",
    "    special_chars = r\"[^a-zA-Z0-9]\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # 1️⃣ Type (placeholder — MUST be dropped before model input)\n",
    "    \n",
    "\n",
    "    # ---------------- URL LEVEL ----------------\n",
    "    features.append(len(url))                              # url_length\n",
    "    features.append(url.count('.'))                        # number_of_dots_in_url\n",
    "    features.append(int(bool(re.search(r'(\\d)\\1+', url)))) # having_repeated_digits_in_url\n",
    "    features.append(sum(c.isdigit() for c in url))         # number_of_digits_in_url\n",
    "    features.append(len(re.findall(special_chars, url)))  # number_of_special_char_in_url\n",
    "    features.append(url.count('-'))                        # number_of_hyphens_in_url\n",
    "    features.append(url.count('_'))                        # number_of_underline_in_url\n",
    "    features.append(url.count('/'))                        # number_of_slash_in_url\n",
    "    features.append(url.count('?'))                        # number_of_questionmark_in_url\n",
    "    features.append(url.count('='))                        # number_of_equal_in_url\n",
    "    features.append(url.count('@'))                        # number_of_at_in_url\n",
    "    features.append(url.count('$'))                        # number_of_dollar_in_url\n",
    "    features.append(url.count('!'))                        # number_of_exclamation_in_url\n",
    "    features.append(url.count('#'))                        # number_of_hashtag_in_url\n",
    "    features.append(url.count('%'))                        # number_of_percent_in_url\n",
    "\n",
    "    # ---------------- DOMAIN LEVEL ----------------\n",
    "    features.append(len(domain))                           # domain_length\n",
    "    features.append(domain.count('.'))                     # number_of_dots_in_domain\n",
    "    features.append(domain.count('-'))                     # number_of_hyphens_in_domain\n",
    "    features.append(int(bool(re.search(special_chars, domain))))  # having_special_characters_in_domain\n",
    "    features.append(len(re.findall(special_chars, domain)))       # number_of_special_characters_in_domain\n",
    "    features.append(int(any(c.isdigit() for c in domain)))        # having_digits_in_domain\n",
    "    features.append(sum(c.isdigit() for c in domain))             # number_of_digits_in_domain\n",
    "    features.append(int(bool(re.search(r'(\\d)\\1+', domain))))     # having_repeated_digits_in_domain\n",
    "\n",
    "    # ---------------- SUBDOMAIN LEVEL ----------------\n",
    "    domain_parts = domain.split('.')\n",
    "    subdomains = domain_parts[:-2] if len(domain_parts) > 2 else []\n",
    "\n",
    "    features.append(len(subdomains))                       # number_of_subdomains\n",
    "    features.append(int(any('.' in s for s in subdomains)))# having_dot_in_subdomain\n",
    "    features.append(int(any('-' in s for s in subdomains)))# having_hyphen_in_subdomain\n",
    "\n",
    "    avg_sub_len = np.mean([len(s) for s in subdomains]) if subdomains else 0\n",
    "    features.append(avg_sub_len)                           # average_subdomain_length\n",
    "\n",
    "    avg_dot = np.mean([s.count('.') for s in subdomains]) if subdomains else 0\n",
    "    features.append(avg_dot)                               # average_number_of_dots_in_subdomain\n",
    "\n",
    "    avg_hyphen = np.mean([s.count('-') for s in subdomains]) if subdomains else 0\n",
    "    features.append(avg_hyphen)                            # average_number_of_hyphens_in_subdomain\n",
    "\n",
    "    features.append(int(any(re.search(special_chars, s) for s in subdomains))) # having_special_characters_in_subdomain\n",
    "    features.append(sum(len(re.findall(special_chars, s)) for s in subdomains))# number_of_special_characters_in_subdomain\n",
    "    features.append(int(any(c.isdigit() for s in subdomains for c in s)))       # having_digits_in_subdomain\n",
    "    features.append(sum(c.isdigit() for s in subdomains for c in s))            # number_of_digits_in_subdomain\n",
    "    features.append(int(any(re.search(r'(\\d)\\1+', s) for s in subdomains)))      # having_repeated_digits_in_subdomain\n",
    "\n",
    "    # ---------------- PATH / QUERY ----------------\n",
    "    features.append(int(bool(path)))                      # having_path\n",
    "    features.append(len(path))                            # path_length\n",
    "    features.append(int(bool(query)))                     # having_query\n",
    "    features.append(int(bool(fragment)))                  # having_fragment\n",
    "    features.append(int('#' in url))                      # having_anchor\n",
    "\n",
    "    # ---------------- ENTROPY ----------------\n",
    "    features.append(shannon_entropy(url))                 # entropy_of_url\n",
    "    features.append(shannon_entropy(domain))              # entropy_of_domain\n",
    "\n",
    "    return np.array(features, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "544d36d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41,)\n"
     ]
    }
   ],
   "source": [
    "features = extract_features_ordered(\n",
    "    \"http://secure-login-google.verify-user.info/login\"\n",
    ")\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a78ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL: http://secure-login-google.verify-user.info/login is predicted as Phishing with Probability 0.9857\n"
     ]
    }
   ],
   "source": [
    "url = \"http://secure-login-google.verify-user.info/login\"\n",
    "features = extract_features_ordered(url)\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_0(features)\n",
    "    prob = torch.sigmoid(logits).item()\n",
    "if prob >= 0.5:\n",
    "    print(f\"The URL: {url} is predicted as Phishing with Probability {prob:.4f}\")\n",
    "else:\n",
    "    print(f\"The URL: {url} is predicted as Legitimate with Probability {1 - prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbb3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
